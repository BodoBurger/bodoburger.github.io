<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.5.0"><meta name=author content="Bodo Burger"><meta name=description content="The book gives an overview of common methods that help to better understand machine learning models. You can support the author by buying the book on leanpub. There is a free online version of the book, too.
Introduction The beginning of the book covers three short stories that illustrate the detrimental consequences of a world controlled by black box machine learning models (1.1). They serve as motivation for why we want to use methods that improve understanding of opaque models."><link rel=alternate hreflang=en-us href=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/><meta name=theme-color content="#d3534f"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/tomorrow-night-eighties.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/tomorrow-night-eighties.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Fira+Sans:400,700%7CMontserrat:300,300italic,400%7CDroid+Sans+Mono&display=swap"><link rel=stylesheet href=/css/academic.min.80abb9afc65400c50ad6171b5e68f22b.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/img/icon-32.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@bododata"><meta property="twitter:creator" content="@bododata"><meta property="og:site_name" content="Bodo Burger"><meta property="og:url" content="https://bodoburger.github.io/post/2019-02-interpretable-ml-book/"><meta property="og:title" content="Interpretable Machine Learning by Christoph Molnar | Bodo Burger"><meta property="og:description" content="The book gives an overview of common methods that help to better understand machine learning models. You can support the author by buying the book on leanpub. There is a free online version of the book, too.
Introduction The beginning of the book covers three short stories that illustrate the detrimental consequences of a world controlled by black box machine learning models (1.1). They serve as motivation for why we want to use methods that improve understanding of opaque models."><meta property="og:image" content="https://bodoburger.github.io/img/icon-192.png"><meta property="twitter:image" content="https://bodoburger.github.io/img/icon-192.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-02-26T00:00:00&#43;00:00"><meta property="article:modified_time" content="2019-12-12T20:56:39&#43;01:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bodoburger.github.io/post/2019-02-interpretable-ml-book/"},"headline":"Interpretable Machine Learning by Christoph Molnar","datePublished":"2019-02-26T00:00:00Z","dateModified":"2019-12-12T20:56:39+01:00","author":{"@type":"Person","name":"Bodo Burger"},"publisher":{"@type":"Organization","name":"Bodo Burger","logo":{"@type":"ImageObject","url":"https://bodoburger.github.io/img/icon-512.png"}},"description":"The book gives an overview of common methods that help to better understand machine learning models. You can support the author by buying the book on leanpub. There is a free online version of the book, too.\nIntroduction The beginning of the book covers three short stories that illustrate the detrimental consequences of a world controlled by black box machine learning models (1.1). They serve as motivation for why we want to use methods that improve understanding of opaque models."}</script><title>Interpretable Machine Learning by Christoph Molnar | Bodo Burger</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/><i class="fas fa-home" aria-hidden=true></i></a><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/aboutme/><span>About me</span></a></li><li class=nav-item><a class=nav-link href=/projects/><span>Projects</span></a></li><li class=nav-item><a class="nav-link active" href=/post><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/notes/><span>Notes</span></a></li><li class=nav-item><a class=nav-link href=/contact/><span>Contact</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article><div class="article-container pt-3"><h1>Interpretable Machine Learning by Christoph Molnar</h1><div class=article-metadata><span class=article-date>Last updated on
2019-12-12</span>
<span class=middot-divider></span><span class=article-reading-time>3 min read</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/categories/book-reviews/>Book reviews</a></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/&amp;text=Interpretable%20Machine%20Learning%20by%20Christoph%20Molnar" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/&amp;t=Interpretable%20Machine%20Learning%20by%20Christoph%20Molnar" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Interpretable%20Machine%20Learning%20by%20Christoph%20Molnar&amp;body=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/&amp;title=Interpretable%20Machine%20Learning%20by%20Christoph%20Molnar" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Interpretable%20Machine%20Learning%20by%20Christoph%20Molnar%20https://bodoburger.github.io/post/2019-02-interpretable-ml-book/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bodoburger.github.io/post/2019-02-interpretable-ml-book/&amp;title=Interpretable%20Machine%20Learning%20by%20Christoph%20Molnar" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style><p>The book gives an overview of common methods
that help to better understand machine learning models.
You can support the author by buying the book on
<a href=https://leanpub.com/interpretable-machine-learning>leanpub</a>.
There is a <a href=https://christophm.github.io/interpretable-ml-book>free online version</a>
of the book, too.</p><div id=introduction class="section level2"><h2>Introduction</h2><p>The beginning of the book covers three short stories
that illustrate the detrimental consequences of
a world controlled by black box machine learning models
(<a href=https://christophm.github.io/interpretable-ml-book/storytime.html>1.1</a>).
They serve as motivation for why we want to use methods
that improve understanding of opaque models.
The ultimate goal of these methods is that a human can understand a model
so that he can consistently predict its results.</p><p><a href=https://christophm.github.io/interpretable-ml-book/interpretability.html>Chapter 2</a>
lays the foundation for the discussion on machine learning interpretability
by answering the following questions:</p><ul><li>Why is interpretation important and when do we need it
(<a href=https://christophm.github.io/interpretable-ml-book/interpretability-importance.html>2.1</a>)?</li><li>How can we classify different interpretation methods
(<a href=https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html>2.2</a>)?<ul><li>intrinsic vs post-hoc methods</li><li>result of the method</li><li>model-specific vs model-agnostic</li><li>local vs global</li></ul></li><li>Which part of a model do we want to inspect
(<a href=https://christophm.github.io/interpretable-ml-book/scope-of-interpretability.html>2.3</a>)?</li><li>How do we evaluate the interpretation
(<a href=https://christophm.github.io/interpretable-ml-book/evaluation-of-interpretability.html>2.4</a>)?</li><li>What does a human need to know to understand a black box model
(<a href=https://christophm.github.io/interpretable-ml-book/explanation.html>2.6</a>)?</li></ul><p>The presented interpretation methods are repeatedly applied to three freely available data sets
representing different kinds of prediction tasks:
<a href=http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset>daily bike rentals</a> (regression),
<a href=https://archive.ics.uci.edu/ml/datasets/Cervical+cancer+%28Risk+Factors%29>cancer risk factors</a> (classification) and
<a href=dcomp.sor.ufscar.br/talmeida/youtubespamcollection/>YouTube spam comments</a> (text classification).
<a href=https://christophm.github.io/interpretable-ml-book/data.html>Chapter 3</a>
introduces the datasets in more detail.</p></div><div id=interpretable-models class="section level2"><h2>Interpretable models</h2><p>This chapter presents models that are interpretable by itself.
For Molnar these are
<a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression</a>,
<a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>,
<a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision trees</a>,
<a href=https://en.wikipedia.org/wiki/Decision_tree#Decision_rules>decision rules</a>,
RuleFit (<a href=http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf>Friedman and Popescu, 2005</a>),
<a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes</a> and
<a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm>k-nearest neighbors</a>.</p><p>These models have in common that the result is accessible to the user without further steps.
They differ in regards to linearity, monotonicity,
the possibility to include feature interactions and the tasks they can handle.</p><p>Molnar concludes the discussion of each method with a comparison of its advantages and disadvantages.
E.g. <a href=https://christophm.github.io/interpretable-ml-book/limo.html#advantages>the apparent simplicity of linear models</a>
is opposed by the difficulty to incorporate nonlinearity, the low predictive performance and
the potentially unintuitive interpretation of the coefficients due to correlated features.</p><p>The following table is an overview of implementations of <em>interpretable models</em>
both for <strong>R</strong> and <strong>Python</strong>:</p><table><tr><th>Method<br></th><th>R</th><th>Python</th></tr><tr><td>linear model</td><td><a href=https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html>lm()</a></td><td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression>sklearn.linear_model.LinearRegression()</a></td></tr><tr><td>logistic model</td><td><a href=https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html>glm(formula, binomial(link = “logit”))</a></td><td><a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression>sklearn.linear_model.LogisticRegression()</a></td></tr><tr><td>decision tree</td><td><a href="https://cran.r-project.org/package=rpart">rpart()</a></td><td><a href=https://scikit-learn.org/stable/modules/tree.html>sklearn.tree</a></tr><tr><td>decision rule</td><td><a href="https://cran.r-project.org/package=OneR">OneR()</a></td><td><a href=https://oracle.github.io/Skater/reference/interpretation.html#bayesian-rule-lists-brl>BRLC() (skater)</a></td
  </tr><tr><td>RuleFit</td><td><a href="https://cran.r-project.org/package=pre">pre()</a></td><td><a href=https://github.com/christophM/rulefit>RuleFit()</a> or
<a href=https://github.com/scikit-learn-contrib/skope-rules>SkopeRules()</a></td></tr><tr><td>Naive Bayes</td><td><a href="https://cran.r-project.org/package=e1071">naiveBayes()</a><td><a href=https://scikit-learn.org/stable/modules/naive_bayes.html>sklearn.naive_bayes</a></td></tr><tr><td>k-nearest neighbors</td><td><a href="https://cran.r-project.org/package=kknn">kknn()</a><td><a href=https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification>sklearn.neighbors</a></td></table></div><div id=model-agnostic-methods class="section level2"><h2>Model-agnostic methods</h2><p>A model-agnostic interpretability method is applied after a model is trained
to make the result more accessible or transparent.
Best case the method is flexible enough to be applied on any model.</p><table><tr><th>Method<br></th><th>R</th><th>Python</th></tr><tr><td>Partial Dependence Plot</td><td>Packages: <a href="https://cran.r-project.org/package=mlr">mlr</a>,
<a href="https://cran.r-project.org/package=pdp">pdp</a>,
<a href="https://cran.r-project.org/package=iml">iml</a>,
<a href=https://github.com/BodoBurger/intame>intame</a></td><td><a href=https://scikit-learn.org/stable/modules/partial_dependence.html>sklearn.inspection.plot_partial_dependence()</a></td></tr><tr><td>Individual Conditional Expectation</td><td>Packages: <a href="https://cran.r-project.org/package=iml">iml</a>,
<a href="https://cran.r-project.org/package=pdp">pdp</a>,
<a href="https://cran.r-project.org/package=ICEbox">ICEbox</a></td><td></td></tr><tr><td>Accumulated Local Effect</td><td>Packages: <a href="https://cran.r-project.org/package=iml">iml</a>,
<a href="https://cran.r-project.org/package=ALEPlot">ALEPlot</a>,
<a href=https://github.com/BodoBurger/intame>intame</a></td><td></td></tr></table></div></div><p class=edit-page><a href=https://github.com/BodoBurger/bodoburger.github.io-hugo/edit/master/content/post/2019-02-interpretable-ml-book/index.html><i class="fas fa-pen pr-2"></i>Edit this page</a></p><div class=article-tags><a class="badge badge-light" href=/tags/interpretable-ml/>Interpretable ML</a>
<a class="badge badge-light" href=/tags/explainable-ai/>explainable AI</a></div><div class="media author-card"><img class="portrait mr-3" src=/authors/admin/avatar_huc489d3e7034bfbd9aa2dbd0934c91cf1_2401155_250x250_fill_q90_lanczos_center.jpg alt=Avatar><div class=media-body><h5 class=card-title><a href=https://bodoburger.github.io>Bodo Burger</a></h5><h6 class=card-subtitle>Statistician, Data Scientist, Machine Learner</h6><p class=card-text>I am a statistician based in Munich, Germany. I hold a Master’s degree in statistics and a Bachelor’s degree in economics.</p><ul class=network-icon aria-hidden=true><li><a href=/contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/bodoburger target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://linkedin.com/in/bodo-burger target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://twitter.com/bododata target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li></ul></div></div></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/R.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/tex.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin=anonymous></script><script>hljs.initHighlightingOnLoad();</script><script>const search_index_filename="/index.json";const i18n={'placeholder':"Search...",'results':"results found",'no_results':"No results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.130521ecfc6f534c52c158217bbff718.js></script><div class=container><footer class=site-footer><p class=powered-by>Bodo Burger 2019 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>